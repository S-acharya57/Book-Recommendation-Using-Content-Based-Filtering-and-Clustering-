{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d6410e-35d7-4323-a1cd-a917bffe9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf4f1105-1ec1-4b89-a75f-863df3cddd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/book_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29181b-7e28-4f34-a7e5-00a10850d5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bc222dd-c4a3-45b4-a0e5-64ec672f3926",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value='', inplace=True)\n",
    "df.drop_duplicates(subset=['book_title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09d42d81-49dc-48db-bfe0-4f55e4afe865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genres'] = df['genres'].apply(lambda x: x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f893d10b-c317-47ce-a809-fe38ce629a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['book_authors'] = df['book_authors'].apply(lambda x:x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e344111e-2913-43e7-97a8-5dec6240985c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['374 pages' '870 pages' '324 pages' ... '1043 pages' '1613 pages'\n",
      " '1776 pages']\n"
     ]
    }
   ],
   "source": [
    "print(df['book_pages'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bbb80d3-be4c-489e-a080-fb3e8bca3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['book_pages'] = pd.to_numeric(df['book_pages'].str.replace(' pages', ''), errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3268da5f-fc3f-4e5d-887c-b059b7708479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['book_pages'] = df['book_pages'].astype(str).str.replace(' pages', '').replace('', np.nan).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0484b88-3dea-444a-8122-5b52f0105257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48483"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['book_title'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034059cc-1f8f-432a-8c0f-c9386c6e0fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94cac9-5322-43f6-9673-be161c687fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5b5b2fa-5079-4509-bb87-98a915fd7c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f1fccc5-fa07-48df-ac83-b18807f0c591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd47ee41-21f3-4af2-bfa9-aa83f3bdda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_desc_vector = vectorizer.fit_transform(df['book_desc'].values.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b954f7-ba3b-460e-b582-23696eb350d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48483, 283853)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_desc_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65dddc89-88b8-44e9-ba5b-9e7bf036e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_in_vectors = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad688768-d4fc-4b90-8e65-2236b5cb2b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(283853,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_in_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc75e470-00c0-40eb-a176-5cf4f01b5661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48483, 48483)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarities = cosine_similarity(books_desc_vector)\n",
    "cos_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1814dcc-efc6-4fbc-bd59-859b01a2511f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.00255534, 0.01704172, ..., 0.        , 0.01387131,\n",
       "        0.0069516 ],\n",
       "       [0.00255534, 1.        , 0.        , ..., 0.        , 0.00974877,\n",
       "        0.        ],\n",
       "       [0.01704172, 0.        , 1.        , ..., 0.        , 0.03106394,\n",
       "        0.02368588],\n",
       "       ...,\n",
       "       [0.01874136, 0.00253853, 0.01766921, ..., 0.        , 0.01297754,\n",
       "        0.0240523 ],\n",
       "       [0.02510485, 0.        , 0.0388411 , ..., 0.        , 0.01524865,\n",
       "        0.        ],\n",
       "       [0.02402623, 0.0142949 , 0.        , ..., 0.        , 0.00201227,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarities[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d7fdf4d-be45-4d1b-be2c-f239abfdb859",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['book_title', 'book_authors', 'book_desc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3046c3f-1683-4380-9441-3aff6d33a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(df2.index, index=df['book_title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf8b2c39-88ea-423f-81eb-ca0bffd38d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_title\n",
       "The Hunger Games                                                                                                   0\n",
       "Harry Potter and the Order of the Phoenix                                                                          1\n",
       "To Kill a Mockingbird                                                                                              2\n",
       "Pride and Prejudice                                                                                                3\n",
       "Twilight                                                                                                           4\n",
       "                                                                                                               ...  \n",
       "Taking the Field: A Fan's Quest to Run the Team He Loves                                                       54296\n",
       "The Baseball Talmud: Koufax, Greenberg, and the Quest for the Ultimate Jewish All-Star Team                    54297\n",
       "Wilpon's Folly - The Story of a Man, His Fortune, and the New York Mets                                        54298\n",
       "He Wanted the Moon: The Madness and Medical Genius of Dr. Perry Baird, and His Daughter's Quest to Know Him    54299\n",
       "The Anthology and the Rise of the Novel: From Richardson to George Eliot                                       54300\n",
       "Length: 48483, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dfab1d8-a4ec-4b59-a339-2a8e8e0e3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_recommendation(title, cos_similarities = cos_similarities):\n",
    "    book_index = indices[title]\n",
    "    similarity_scores = list(enumerate(cos_similarities[book_index]))\n",
    "    similarity_scores = sorted(similarity_scores, key = lambda x:x[1], reverse = True)\n",
    "    top_books = [i[0] for i in similarity_scores[1:5]]\n",
    "    recommended_books = df2.iloc[top_books]['book_title'].tolist()\n",
    "    return recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c000432c-d0ec-4849-9a0a-d83e91911747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Snow Ride', 'Premeditated', 'Hiding in the Shadows', 'The Rich Are Different']\n"
     ]
    }
   ],
   "source": [
    "recs = give_recommendation('1984')\n",
    "print(recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee80519-9d80-49fe-88d2-b2d191a2a84c",
   "metadata": {},
   "source": [
    "### Splitting the genres separated by '|' into a list of genres "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea8c0c4e-64af-490d-8a25-f04464c48235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book_authors', 'book_desc', 'book_edition', 'book_format', 'book_isbn',\n",
       "       'book_pages', 'book_rating', 'book_rating_count', 'book_review_count',\n",
       "       'book_title', 'genres', 'image_url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c12a557-f797-4594-9bc7-5344b51d035d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "779200eb-e18c-49ed-80b5-71554129fefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCDEFGHIJKLMNOPQRSTUVWXYZ'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fccf74a4-2d05-4a9d-bef8-87c180ec4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_feature'] = ['name_{}'.format(x) for x in df['book_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9da5ef8c-8abb-46ba-b18c-9b12dd77b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_text_to_feature(s, sep=' ', join_sep='x', to_include=string.ascii_lowercase, to_include2 = string.ascii_uppercase):\n",
    "    def filter_word(word):\n",
    "        return ''.join([c for c in word if (c in to_include or to_include2)])\n",
    "    return join_sep.join([filter_word(word) for word in s.split(sep)])\n",
    "\n",
    "df['name_feature'] = df['book_title'].apply(raw_text_to_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e647f5d7-b6d8-4f87-b50e-a201940f2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['corpus'] = (pd.Series(df[['book_desc', 'name_feature']]\n",
    "                .fillna('')\n",
    "                .values.tolist()\n",
    "                ).str.join(' '))\n",
    "df['corpus'] = df['corpus'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98ff708e-d28b-4565-ba65-ecf2fa2d9a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48483, 48483)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corpus'].count(), df['name_feature'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b03e5096-0bbc-445f-aea4-4ee1f8fec59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48483"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['corpus'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9c785-4176-4bf5-8751-472cee198888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "271b228a-a90f-44c3-894f-addd1cdc732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((312839,), (48483, 312839))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(stop_words = 'english')\n",
    "books_desc_vector2 = vectorizer.fit_transform(df['corpus'].values.astype(str))\n",
    "all_in_vectors2 = vectorizer.get_feature_names_out()\n",
    "all_in_vectors2.shape, books_desc_vector2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4510b8b6-d3d6-4ad3-bec4-a3b872cbbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarities2 = cosine_similarity(books_desc_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "526a4302-f2f9-48e3-b91c-d177b48d2059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48483, 48483)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarities2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59721f2e-412a-425b-a991-cdbf658bd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_recommendation_name_desc(title, cos_similarities = cos_similarities2):\n",
    "    book_index = indices[title]\n",
    "    similarity_scores = list(enumerate(cos_similarities[book_index]))\n",
    "    similarity_scores = sorted(similarity_scores, key = lambda x:x[1], reverse = True)\n",
    "    top_books = [i[0] for i in similarity_scores[1:10]]\n",
    "    recommended_books = [df2.iloc[top_books]['book_title'].tolist()]\n",
    "    print(type(recommended_books[0]))\n",
    "    res = []\n",
    "    for i, book in enumerate(recommended_books[0]):\n",
    "        res.append((book, similarity_scores[i][-1]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ea49035-0359-435b-9bf0-5ce881c992b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[('The Sea, the Sea', 1.0), ('Notorious Nora', 0.3004609644286415), (\"It's Not about the Bike: My Journey Back to Life\", 0.14122755858510808), ('Just Ella', 0.1268142394345682), (\"A Hustler's Promise 2\", 0.12224691643904045), ('Rush', 0.11534686406408201), ('الحب في المنفى', 0.11319512799862641), ('The Criminal Mastermind Collection, Bks 1-3', 0.11239470741718388), ('Romiette and Julio', 0.11014456050110508)]\n"
     ]
    }
   ],
   "source": [
    "recs2 = give_recommendation_name_desc('To Kill a Mockingbird')\n",
    "print(recs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e2f3651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " scipy.sparse.csr.csr_matrix,\n",
       " numpy.ndarray,\n",
       " scipy.sparse.csr.csr_matrix)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cos_similarities), type(books_desc_vector), type(cos_similarities2), type(books_desc_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48872653",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This sheet is too large! Your sheet size is: 48483, 48483 Max sheet size is: 1048576, 16384",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Convert the numpy arrays to dataframes and save to separate Excel files\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m pd\u001b[39m.\u001b[39;49mDataFrame(cos_similarities)\u001b[39m.\u001b[39;49mto_excel(\u001b[39m'\u001b[39;49m\u001b[39mcos_similarities.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      3\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(books_desc_vector\u001b[39m.\u001b[39mtodense())\u001b[39m.\u001b[39mto_excel(\u001b[39m'\u001b[39m\u001b[39mvectorized_books_data.xlsx\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\sajjan\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py:2284\u001b[0m, in \u001b[0;36mNDFrame.to_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes, storage_options)\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mformats\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexcel\u001b[39;00m \u001b[39mimport\u001b[39;00m ExcelFormatter\n\u001b[0;32m   2273\u001b[0m formatter \u001b[39m=\u001b[39m ExcelFormatter(\n\u001b[0;32m   2274\u001b[0m     df,\n\u001b[0;32m   2275\u001b[0m     na_rep\u001b[39m=\u001b[39mna_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2282\u001b[0m     inf_rep\u001b[39m=\u001b[39minf_rep,\n\u001b[0;32m   2283\u001b[0m )\n\u001b[1;32m-> 2284\u001b[0m formatter\u001b[39m.\u001b[39;49mwrite(\n\u001b[0;32m   2285\u001b[0m     excel_writer,\n\u001b[0;32m   2286\u001b[0m     sheet_name\u001b[39m=\u001b[39;49msheet_name,\n\u001b[0;32m   2287\u001b[0m     startrow\u001b[39m=\u001b[39;49mstartrow,\n\u001b[0;32m   2288\u001b[0m     startcol\u001b[39m=\u001b[39;49mstartcol,\n\u001b[0;32m   2289\u001b[0m     freeze_panes\u001b[39m=\u001b[39;49mfreeze_panes,\n\u001b[0;32m   2290\u001b[0m     engine\u001b[39m=\u001b[39;49mengine,\n\u001b[0;32m   2291\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   2292\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sajjan\\anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\io\\formats\\excel.py:823\u001b[0m, in \u001b[0;36mExcelFormatter.write\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options)\u001b[0m\n\u001b[0;32m    821\u001b[0m num_rows, num_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mshape\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m num_rows \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_rows \u001b[39mor\u001b[39;00m num_cols \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_cols:\n\u001b[1;32m--> 823\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    824\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis sheet is too large! Your sheet size is: \u001b[39m\u001b[39m{\u001b[39;00mnum_rows\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mnum_cols\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    825\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMax sheet size is: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_rows\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_cols\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    826\u001b[0m     )\n\u001b[0;32m    828\u001b[0m formatted_cells \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_formatted_cells()\n\u001b[0;32m    829\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(writer, ExcelWriter):\n",
      "\u001b[1;31mValueError\u001b[0m: This sheet is too large! Your sheet size is: 48483, 48483 Max sheet size is: 1048576, 16384"
     ]
    }
   ],
   "source": [
    "# Convert the numpy arrays to dataframes and save to separate Excel files\n",
    "pd.DataFrame(cos_similarities).to_excel('cos_similarities.xlsx', index=False)\n",
    "pd.DataFrame(books_desc_vector.todense()).to_excel('vectorized_books_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = pd.read_excel('cos_similarities.xlsx').values.squeeze()\n",
    "sp_mat1 = sp.csr_matrix(pd.read_excel('vectorized_books_data.xlsx').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c442b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the numpy arrays from the Excel files\n",
    "arr1 = pd.read_excel('array1.xlsx').values.squeeze()\n",
    "arr2 = pd.read_excel('array2.xlsx').values.squeeze()\n",
    "sp_mat1 = sp.csr_matrix(pd.read_excel('sparse_mat1.xlsx').values)\n",
    "sp_mat2 = sp.csr_matrix(pd.read_excel('sparse_mat2.xlsx').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse matrices to dataframes and save to separate Excel files\n",
    "pd.DataFrame(sp_mat1.todense()).to_excel('sparse_mat1.xlsx', index=False)\n",
    "pd.DataFrame(sp_mat2.todense()).to_excel('sparse_mat2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a3ed3f7-4c99-489f-8da7-db1f62dd60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'cos_similarities':cos_similarities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e95b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'books_TFID_vectorized':books_desc_vector,\n",
    "        'cos_similarities_title_desc':cos_similarities2,\n",
    "        'books_TFID_vectorized_title_desc':books_desc_vector2,\n",
    "        'df_corpus': df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "672c87e4-fd5f-422d-8df4-0bf517958c78",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mcos_sim_basic_50000.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 4\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(data, f)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cos_sim_basic_50000.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f1dc3b-421f-46b5-adae-540bbcce020b",
   "metadata": {},
   "source": [
    "## Considering other than book_desc for content based recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81deadc5-c98e-4db5-abf6-1f84982d01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3b48d4-ef6d-4f0a-ae59-0de24fa00a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['book_title', 'book_authors', 'book_desc', 'book_pages', 'book_review_count']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8461ab6b-bf10-4967-b050-2b20e731000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_pipeline = Pipeline([('vect', CountVectorizer(stop_words='english'))])\n",
    "pages_pipeline = Pipeline([('scaler', StandardScaler())])\n",
    "authors_pipeline = Pipeline([('tolist', ListToStringTransformer()),\n",
    "                             ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "reviewcount_pipeline = Pipeline([('scaler', StandardScaler())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feea98f3-72b1-470b-b923-0eb9370cee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListToStringTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [' '.join(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c2b86-aafb-4555-9426-bb01ecef2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['book_pages'] = df['book_pages'].reshape(-1, 1)\n",
    "df['book_pages'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e5aeb-e4ea-4912-a232-6f18b192ebdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227fea86-d8a9-4596-99ec-9555d88ff426",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([('desc', desc_pipeline, ['book_desc']),\n",
    "                                  ('pages', pages_pipeline, ['book_pages']),\n",
    "                                  ('authors', authors_pipeline, ['book_authors']),\n",
    "                                  ('reviewcount', reviewcount_pipeline, ['book_review_count'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d802372-4eab-48ea-9866-6ec160958613",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = preprocessor.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49e87e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbaaa874-82b6-4b20-9164-63dfefd4bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sajjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# transform the genres into a TF-IDF matrix\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(df[\u001b[39m'\u001b[39;49m\u001b[39mgenres\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     17\u001b[0m \u001b[39m# perform PCA to reduce the dimensionality of the TF-IDF matrix\u001b[39;00m\n\u001b[0;32m     18\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sajjan\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2073\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2074\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2075\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2076\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2077\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2078\u001b[0m )\n\u001b[1;32m-> 2079\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2080\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2081\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sajjan\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sajjan\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1228\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1226\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1228\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1229\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1230\u001b[0m         )\n\u001b[0;32m   1232\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download the stop words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# remove the stop words from the documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['genres'] = df['genres'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
    "\n",
    "# create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# transform the genres into a TF-IDF matrix\n",
    "X = vectorizer.fit_transform(df['genres'])\n",
    "\n",
    "# perform PCA to reduce the dimensionality of the TF-IDF matrix\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "# assign a color to each genre\n",
    "colors = plt.cm.tab10(range(len(genres)))\n",
    "\n",
    "# create a scatter plot with the genre as the color\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "for i, genre in enumerate(genres):\n",
    "    mask = df['genres'].str.contains(genre)\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], c=colors[i], label=genre, alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "# set the labels\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Clusters based on similar genres')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6c035ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'g', 'Z', '.', '9', 'G', 'U', 'b', 'Q', 'N', '8', '0', 'l', 'w', 'R', 'z', 'K', 'p', 'r', 'J', '6', '1', 'e', 'L', '5', 'B', '4', 'q', 'E', 'c', 'j', 'u', 'H', '3', 'P', 'F', '2', 'C', 'v', 'é', '7', 'W', 'f', 'x', 'X', 'n', 'k', 'V', 'h'}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m# transform the genres into a TF-IDF matrix\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[39m=\u001b[39m TfidfVectorizer(stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39;49mfit_transform(df[\u001b[39m'\u001b[39;49m\u001b[39mgenres\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      8\u001b[0m \u001b[39m# perform PCA to reduce the dimensionality of the TF-IDF matrix\u001b[39;00m\n\u001b[0;32m      9\u001b[0m pca \u001b[39m=\u001b[39m PCA(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sajjan\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2073\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2074\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2075\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2076\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2077\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2078\u001b[0m )\n\u001b[1;32m-> 2079\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2080\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2081\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sajjan\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1335\u001b[0m             )\n\u001b[0;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sajjan\\anaconda3\\envs\\tf\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1228\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1226\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1228\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1229\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1230\u001b[0m         )\n\u001b[0;32m   1232\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "unique_genres = set(' '.join(df['genres']).split())\n",
    "print(unique_genres)\n",
    "\n",
    "# transform the genres into a TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['genres'])\n",
    "\n",
    "# perform PCA to reduce the dimensionality of the TF-IDF matrix\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X.toarray())\n",
    "\n",
    "# create a scatter plot of the principal components\n",
    "plt.scatter(principal_components[:,0], principal_components[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a40825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "a50afa644c776f7a2a60a52e60e30bd38a1de8f1520052de409d0e9c4f2415b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
